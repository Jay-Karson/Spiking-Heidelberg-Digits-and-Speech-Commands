{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "from snntorch import utils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import snntorch.spikeplot as splt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the data\n",
    "# fileh = tables.open_file(\"hdspikes/shd_train.h5\", mode='r')\n",
    "# units = fileh.root.spikes.units\n",
    "# times = fileh.root.spikes.times\n",
    "# labels = fileh.root.labels\n",
    "# root\n",
    "# |-spikes\n",
    "#    |-times[]\n",
    "#    |-units[]\n",
    "# |-labels[]\n",
    "# |-extra\n",
    "#    |-speaker[]\n",
    "#    |-keys[]\n",
    "#    |-meta_info\n",
    "#       |-gender[]\n",
    "#       |-age[]\n",
    "#       |-body_height[]\n",
    "# Training Parameters\n",
    "batch_size = 32\n",
    "# Torch Variables\n",
    "dtype = torch.float\n",
    "# Check whether a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")     \n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomHDF5Dataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.fileh = h5py.File(file_path, 'r')\n",
    "        self.units = self.fileh['spikes']['units']\n",
    "        self.times = self.fileh['spikes']['times']\n",
    "        self.labels = self.fileh['labels']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        unit_data = torch.tensor(self.units[index], dtype=torch.float32)\n",
    "        time_data = torch.tensor(self.times[index], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[index], dtype=torch.long)\n",
    "\n",
    "        # ensures size is always 700 (to prevent different sized tensors error\n",
    "        # change size as needed\n",
    "        # it matches size in num_inputs\n",
    "        target_length = 700\n",
    "        if unit_data.size(0) < target_length:\n",
    "            pad_length_unit = target_length - unit_data.size(0)\n",
    "            unit_data = torch.nn.functional.pad(unit_data, (0, pad_length_unit), mode='constant', value=0)\n",
    "        elif unit_data.size(0) > target_length:\n",
    "            unit_data = unit_data[:target_length]\n",
    "        \n",
    "        if time_data.size(0) < target_length:\n",
    "            pad_length_time = target_length - time_data.size(0)\n",
    "            time_data = torch.nn.functional.pad(time_data, (0, pad_length_time), mode='constant', value=0)\n",
    "        elif time_data.size(0) > target_length:\n",
    "            time_data = time_data[:target_length]\n",
    "        \n",
    "        return unit_data, time_data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have correct file path\n",
    "train_dataset = CustomHDF5Dataset(\"hdspikes/shd_train.h5\")\n",
    "test_dataset = CustomHDF5Dataset(\"hdspikes/shd_test.h5\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# from https://github.com/fzenke/spytorch/blob/main/notebooks/SpyTorchTutorial4.ipynb\n",
    "# Network Architecture Parameters\n",
    "num_inputs = 700\n",
    "num_hidden = 100\n",
    "num_outputs = 20\n",
    "\n",
    "# Temporal Dynamics\n",
    "num_steps = 25\n",
    "time_step = 1e-3\n",
    "tau_mem = 10e-3\n",
    "tau_syn = 5e-3\n",
    "alpha = float(np.exp(-time_step/tau_syn))\n",
    "beta = float(np.exp(-time_step/tau_mem))\n",
    "lif1 = snn.Leaky(beta=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        combined_input_size = 2 * num_inputs  # since we're concatenating unit_data and time_data\n",
    "\n",
    "        self.fc1 = nn.Linear(combined_input_size, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, unit_data, time_data):\n",
    "        # Concatenate unit_data and time_data\n",
    "        x = torch.cat((unit_data, time_data), dim=1)\n",
    "\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            cur1 = self.fc1(x)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
    "\n",
    "\n",
    "net = Net().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy(mem_rec, targets, num_steps, batch_size):\n",
    "    # Initialize correct predictions counter\n",
    "    correct_predictions = 0\n",
    "\n",
    "    # Iterate over each time step's predictions\n",
    "    for step in range(num_steps):\n",
    "        # Obtain predicted labels from the membrane potentials at each time step\n",
    "        _, predicted_labels = torch.max(mem_rec[step], dim=1)\n",
    "        # Check how many predictions match the ground truth labels\n",
    "        correct_predictions += (predicted_labels == targets).sum().item()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / (num_steps * batch_size)\n",
    "\n",
    "    # Print accuracy\n",
    "    print(f\"Accuracy on this batch: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get first batch of data\n",
    "data_iter = iter(train_loader)\n",
    "unit_data, time_data, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 32, 20])\n"
     ]
    }
   ],
   "source": [
    "# Reshape and move data to the correct device\n",
    "unit_data = unit_data.view(unit_data.size(0), -1).to(device)\n",
    "time_data = time_data.view(time_data.size(0), -1).to(device)\n",
    "targets = targets.to(device)\n",
    "\n",
    "spk_rec, mem_rec = net(unit_data, time_data)\n",
    "print(mem_rec.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 83.497\n",
      "Accuracy on this batch: 2.88%\n"
     ]
    }
   ],
   "source": [
    "# initialize the total loss value\n",
    "loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "# sum loss at every step\n",
    "for step in range(num_steps):\n",
    "  loss_val += loss(mem_rec[step], targets)\n",
    "\n",
    "print(f\"Training loss: {loss_val.item():.3f}\")\n",
    "\n",
    "# Initialize correct predictions counter\n",
    "correct_predictions = 0\n",
    "\n",
    "# Iterate over each time step's predictions\n",
    "for step in range(num_steps):\n",
    "    # Obtain predicted labels from the membrane potentials at each time step\n",
    "    _, predicted_labels = torch.max(mem_rec[step], dim=1)\n",
    "    # Check how many predictions match the ground truth labels\n",
    "    correct_predictions += (predicted_labels == targets).sum().item()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions / (num_steps * batch_size)\n",
    "print_accuracy(mem_rec, targets, num_steps, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear previously stored gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# calculate the gradients\n",
    "loss_val.backward()\n",
    "\n",
    "# weight update\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 81.158\n",
      "Accuracy on this batch: 2.75%\n"
     ]
    }
   ],
   "source": [
    "# now rerun after a single iteration\n",
    "spk_rec, mem_rec = net(unit_data.view(batch_size, -1), time_data.view(batch_size, -1))\n",
    "\n",
    "# Initialize the total loss value again for the new outputs\n",
    "loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "\n",
    "# Sum loss at every step for the new outputs\n",
    "for step in range(num_steps):\n",
    "    loss_val += loss(mem_rec[step], targets)\n",
    "\n",
    "print(f\"Training loss: {loss_val.item():.3f}\")\n",
    "print_accuracy(mem_rec, targets, num_steps, batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 65.492\n",
      "Accuracy on this batch: 14.00%\n"
     ]
    }
   ],
   "source": [
    "# everytime this cell runs accuracy goes up\n",
    "# example: i ran it 10 times accuracy went from 9% to 11%\n",
    "\n",
    "# Clear previously stored gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Calculate the gradients\n",
    "loss_val.backward()\n",
    "\n",
    "# Weight update\n",
    "optimizer.step()\n",
    "\n",
    "spk_rec, mem_rec = net(unit_data.view(batch_size, -1), time_data.view(batch_size, -1))\n",
    "\n",
    "# Initialize the total loss value again for the new outputs\n",
    "loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "\n",
    "# Sum loss at every step for the new outputs\n",
    "for step in range(num_steps):\n",
    "    loss_val += loss(mem_rec[step], targets)\n",
    "\n",
    "print(f\"Training loss: {loss_val.item():.3f}\")\n",
    "print_accuracy(mem_rec, targets, num_steps, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "snntorch_tutorial_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
